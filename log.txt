一次典型的 PyTorch 方法调用的 trace：                 
                                            
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 378, in __init__
[rank0]:     self.capture()                         
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 497, in capture
[rank0]:     ) = self.capture_one_batch_size(bs, forward)
[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 676, in capture_one_batch_size
[rank0]:     run_once()                             
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/model_executor/cuda_graph_runner.py", line 663, in run_once
[rank0]:     logits_output_or_pp_proxy_tensors = forward(
[rank0]:                                         ^^^^^^^^
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)           
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^           
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/models/llama.py", line 469, in forward
[rank0]:     hidden_states = self.model(            
[rank0]:                     ^^^^^^^^^^^            
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)   
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^   
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/models/llama.py", line 342, in forward
[rank0]:     hidden_states, residual = layer(       
[rank0]:                               ^^^^^^       
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)   
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^       
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/models/llama.py", line 266, in forward
[rank0]:     hidden_states = self.self_attn(            
[rank0]:                     ^^^^^^^^^^^^^^^            
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)    
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)       
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^       
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/models/llama.py", line 194, in forward
[rank0]:     qkv, _ = self.qkv_proj(hidden_states)      
[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^      
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)    
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^    
[rank0]:   File "/home/yzy/rebuild-pytorch/pytorch/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)       
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^       
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/layers/linear.py", line 427, in forward
[rank0]:     output_parallel = self.quant_method.apply(self, input_, bias)
[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/home/yzy/rebuild-pytorch/sglang/python/sglang/srt/layers/quantization/unquant.py", line 131, in apply
[rank0]:     return F.linear(x, layer.weight, bias) 
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 